# SFT (Supervised Fine-Tuning) Configuration
# -------------------------------------------

model:
  # Qwen3-4B model (instruction-tuned variant)
  # Note: Qwen3 uses "Qwen/Qwen3-4B" naming (no "-Instruct" suffix)
  name: "Qwen/Qwen3-4B"
  # Disable thinking mode for Qwen3 (only applies when using chat template)
  extra_chat_template_kwargs:
    enable_thinking: false
  # GPU selection (sets CUDA_VISIBLE_DEVICES before importing torch):
  #   - "auto": use all available GPUs (default)
  #   - "cuda:0": use only GPU 0
  #   - "cuda:2": use only GPU 2
  #   - [2, 7]: use GPUs 2 and 7 for model parallelism
  device: [6]

training:
  num_epochs: 1
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  max_seq_length: 1024       # set high to avoid truncation; Qwen3 supports 32K
  bf16: true
  logging_steps: 10
  save_strategy: "epoch"
  seed: 42
  gradient_checkpointing: true
