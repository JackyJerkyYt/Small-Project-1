# GRPO (Group Relative Policy Optimization) Configuration
# -------------------------------------------------------

model:
  # Qwen3-4B model (instruction-tuned variant)
  # Note: Qwen3 uses "Qwen/Qwen3-4B" naming (no "-Instruct" suffix)
  name: "Qwen/Qwen3-4B"
  # Disable thinking mode for Qwen3 (only applies when using chat template)
  extra_chat_template_kwargs:
    enable_thinking: false
  # GPU selection:
  #   - "auto": use all available GPUs (default)
  #   - "cuda:0": use only GPU 0
  #   - "cuda:1": use only GPU 1
  #   - [0, 1]: use GPUs 0 and 1 for model parallelism
  #   - null: let device_map="auto" decide
  device: "auto"

training:
  num_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-6
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  max_seq_length: 4096       # set high to avoid truncation; Qwen3 supports 32K
  bf16: true
  logging_steps: 10
  save_strategy: "epoch"
  seed: 42
  gradient_checkpointing: true

grpo:
  num_generations: 4
  max_new_tokens: 2048       # max tokens the model can generate per completion
  max_prompt_length: 4096    # set high to avoid truncation; GSM8K prompts are short
  temperature: 0.7
  beta: 0.1
