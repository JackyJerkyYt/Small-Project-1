# DPO (Direct Preference Optimization) Configuration

model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  device: [3, 5]

training:
  num_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16   # effective batch = 2 x 16 = 32 pairs = 64 sequences
  learning_rate: 5.0e-6            # match GRPO for comparability
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  bf16: true
  logging_steps: 10
  save_strategy: "epoch"
  seed: 42
  gradient_checkpointing: true

dpo:
  beta: 0.1                    # KL penalty coefficient
  max_length: 2048             # max total sequence length (prompt + response)
  max_prompt_length: 512
  loss_type: "sigmoid"         # "sigmoid" (standard DPO) or "ipo"
  num_generations: 8           # completions to sample per prompt for pair construction
  generation_batch_size: 8     # prompts per batch during generation (each generates num_generations sequences)
  generation_temperature: 0.7  # temperature during pair generation
  max_new_tokens: 1024         # max tokens per generation during pair construction
  min_correct: 8               # min correct completions across batch to keep it
  min_incorrect: 8             # min incorrect completions across batch to keep it
  num_pairs_per_batch: 32      # pairs to sample from each valid batch